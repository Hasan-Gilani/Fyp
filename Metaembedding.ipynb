{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5) MetaEmbedding-4-withCBOW-WebKB.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-45a1F0TtFSF",
        "Ugh5388ClOLh",
        "u1EcVkJs6QzO",
        "atHxliHURdoz",
        "8dMh9gvRLu02",
        "4fmsGiI9fVal",
        "Nn6Y9WZhtTzq",
        "lVWKN0uUtjxp",
        "U4KfhQsgu4WF"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-45a1F0TtFSF"
      },
      "source": [
        "# Imports and Downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-EAuoYS0WZD",
        "outputId": "7499936e-9d1d-4588-86e6-dcd415d84d2e"
      },
      "source": [
        "# A\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords \n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "wordnet_lemmatizer = WordNetLemmatizer() \n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import sys\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import calinski_harabasz_score\n",
        "from collections import Counter\n",
        "\n",
        "import gensim.downloader as api\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq4gJ9cTdP-q"
      },
      "source": [
        "# B\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugh5388ClOLh"
      },
      "source": [
        "# Files Reading, Pre-Processing & Saving for Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63WBuaeUCS4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3d82f9e-df34-430a-9cc0-33c7ee98efa6"
      },
      "source": [
        "# C\n",
        "!unzip './drive/MyDrive/DatasetsZip/webkb'\n",
        "!unzip './drive/MyDrive/DatasetsZip/20news-18828'\n",
        "!unzip './drive/MyDrive/DatasetsZip/reuters'\n",
        "!unzip './drive/MyDrive/DatasetsZip/news Ag'\n",
        "!unzip './drive/MyDrive/DatasetsZip/bbc news'\n",
        "!unzip './bbc news/BBC News Test.csv'\n",
        "!unzip './bbc news/BBC News Train.csv'\n",
        "!unzip './drive/MyDrive/DatasetsZip/webkb2'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open ./drive/MyDrive/DatasetsZip/reuters, ./drive/MyDrive/DatasetsZip/reuters.zip or ./drive/MyDrive/DatasetsZip/reuters.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_XxhXuel05Q"
      },
      "source": [
        "# WebKB Dataset\n",
        "\n",
        "webkbData = []\n",
        "\n",
        "path = './webkb'\n",
        "for r, d, f in os.walk(path):\n",
        "\n",
        "  for file in f:\n",
        "      file_temp = \"\"\n",
        "      p = os.path.join(r, file)\n",
        "      file_temp = open(p,'r', errors='ignore').read()\n",
        "      soup = BeautifulSoup(file_temp, 'html.parser')\n",
        "      body = soup.text\n",
        "      body = str(body)\n",
        "      # body = str(np.char.lower(body))\n",
        "      webkbData.append(body)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOelCN1B7YPv"
      },
      "source": [
        "# News20 Dataset\n",
        "\n",
        "news20Data = []\n",
        "\n",
        "path = './20news-18828'\n",
        "for r, d, f in os.walk(path):\n",
        "\n",
        "  for file in f:\n",
        "      file_temp = \"\"\n",
        "      p = os.path.join(r, file)\n",
        "      file_temp = open(p,'r', errors='ignore').read()\n",
        "      body = str(file_temp)\n",
        "      # body = str(np.char.lower(file_temp))\n",
        "      news20Data.append(body)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVi8ZW9OLGVl"
      },
      "source": [
        "# Reuters Dataset\n",
        "\n",
        "reutersData = []\n",
        "\n",
        "path = './reuters'\n",
        "for r, d, f in os.walk(path):\n",
        "\n",
        "  for file in f:\n",
        "      file_temp = \"\"\n",
        "      p = os.path.join(r, file)\n",
        "      file_temp = open(p,'r', errors='ignore').read()\n",
        "      body = str(file_temp)\n",
        "      # body = str(np.char.lower(file_temp))\n",
        "      reutersData.append(body)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcaPKOPLGZEf"
      },
      "source": [
        "# AG-News Dataset\n",
        "\n",
        "train = pd.read_csv('./train.csv')\n",
        "test = pd.read_csv('./test.csv')\n",
        "\n",
        "train['content'] = train['Title']+' '+train['Description']\n",
        "test['content'] = test['Title']+' '+test['Description']\n",
        "\n",
        "ag_newsData = list(train['content']) + list(test['content'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI2RlQSEGY7h"
      },
      "source": [
        "# BBC-News Dataset\n",
        "\n",
        "train = pd.read_csv('./BBC News Train.csv')\n",
        "test = pd.read_csv('./BBC News Test.csv')\n",
        "\n",
        "bbc_newsData = list(train['Text']) + list(test['Text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx9KfnteLrhh"
      },
      "source": [
        "rawDataset = webkbData + news20Data + reutersData + ag_newsData + bbc_newsData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdFY1bTtKRum",
        "outputId": "fcd5bbb6-dc93-49f4-84af-d8b6a4d546c0"
      },
      "source": [
        "len(rawDataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "167726"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peCO4vQcirMi"
      },
      "source": [
        "# Pre-processing\n",
        "\n",
        "# test_list = ['Date: ', 'Server: ', 'Content-type: ', 'Content-length: ', 'Last-modified: ', 'From: ', 'Subject: '] \n",
        "test_list = [] \n",
        "# remove punctuation from each word\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "Dataset=[]\n",
        "for doc in rawDataset: \n",
        "  lines = list(filter(bool, doc.splitlines()))\n",
        "  temp_line=\"\"\n",
        "  for line in lines:\n",
        "    if not any(map(line.__contains__, test_list)):\n",
        "      # convert to lower case\n",
        "      line = str(np.char.lower(line))\n",
        "      line = re.sub(r'\\d+', '', line)\n",
        "      # text_tokens = word_tokenize(line)\n",
        "      text_tokens = re.split(r'\\W+', line)\n",
        "      for word in text_tokens: \n",
        "          # remove remaining tokens that are not alphabetic and filter out sopwords\n",
        "          if ((word not in stop_words) and (word.isalpha())): \n",
        "              # temp_line = temp_line + \" \" + wordnet_lemmatizer.lemmatize(word)\n",
        "              temp_line = temp_line + \" \" + word.translate(table)\n",
        "  Dataset.append(temp_line.strip()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "gObbO2FbPRcP",
        "outputId": "fbab64cf-cf1b-4a75-cddb-11c3501a92ce"
      },
      "source": [
        "rawDataset[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Date: Tue, 10 Dec 1996 17:53:27 GMT\\nServer: NCSA/1.4.2\\nContent-type: text/html\\nLast-modified: Fri, 07 Jun 1996 23:08:01 GMT\\nContent-length: 323\\n\\n\\nShopBot\\n\\nThe ShopBot Has Moved\\n\\nIf your browser supports Java, please use the \\nnew version here.\\n\\nIf your browser doesn't support Java, you'll have to resort to the \\nold out-of-date version here.\\n\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "t6sCZkoPPS5u",
        "outputId": "356c2f4b-f40d-47fa-d2b6-74cd1ce953c0"
      },
      "source": [
        "Dataset[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'date tue dec gmt server ncsa content type text html last modified fri jun gmt content length shopbot shopbot moved browser supports java please use new version browser support java resort old date version'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1EcVkJs6QzO"
      },
      "source": [
        "# CBOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co0IzP9B_cMB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a62d22db-2197-4379-9e07-fd539069adfd"
      },
      "source": [
        "word2vec_model = api.load('word2vec-google-news-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MJb5sYBCdxc"
      },
      "source": [
        "print(word2vec_model.most_similar(\"computer\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gelLqpJInHD"
      },
      "source": [
        "print(len(word2vec_model[\"cat\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atHxliHURdoz"
      },
      "source": [
        "# GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UlYzW0tCheS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ef4489-d5ec-49b0-c2e3-9b48aac579b8"
      },
      "source": [
        "glove_model = api.load('glove-wiki-gigaword-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91uahq0XK3fH",
        "outputId": "18fab901-5986-45e9-bfa6-bdac56161f18"
      },
      "source": [
        "print(glove_model.most_similar(\"computer\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('computers', 0.8248153924942017),\n",
              " ('software', 0.733441948890686),\n",
              " ('pc', 0.6240140199661255),\n",
              " ('technology', 0.6198545694351196),\n",
              " ('computing', 0.6178765296936035),\n",
              " ('laptop', 0.5955508947372437),\n",
              " ('internet', 0.5857782363891602),\n",
              " ('ibm', 0.5825320482254028),\n",
              " ('systems', 0.574499249458313),\n",
              " ('hardware', 0.5728795528411865)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oq-OkOptC_yq",
        "outputId": "9d3b28df-2657-444f-cb5d-4cceaac2c5f5"
      },
      "source": [
        "print(len(glove_model[\"cat\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dMh9gvRLu02"
      },
      "source": [
        "# Universal Sentence Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB_VLrS0QjcA"
      },
      "source": [
        "u_s_e_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "print(len(u_s_e_model([\"computer\"])[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fmsGiI9fVal"
      },
      "source": [
        "# FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzvmH_aHmL1j",
        "outputId": "87ce84f9-16fc-42ec-f8ff-259d14a450ca"
      },
      "source": [
        "# D\n",
        "!git clone https://github.com/facebookresearch/fastText.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'fastText' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YcoH_Ffm0Zx",
        "outputId": "ded21fd5-323b-4a73-bd63-9a750c5de5c7"
      },
      "source": [
        "# E\n",
        "cd fastText"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fastText\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aozVnVYim3xr",
        "outputId": "d5234d67-5128-4a71-fa7c-0682be939186"
      },
      "source": [
        "# F\n",
        "!sudo pip install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /content/fastText\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (2.6.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (56.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3086290 sha256=4fa731f326f607bb6760ced9256df73147b968ca865906e6fd8afd82d3185898\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ms1f32rj/wheels/a1/9f/52/696ce6c5c46325e840c76614ee5051458c0df10306987e7443\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "  Found existing installation: fasttext 0.9.2\n",
            "    Uninstalling fasttext-0.9.2:\n",
            "      Successfully uninstalled fasttext-0.9.2\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMiaGuqPn5pL"
      },
      "source": [
        "f = open(\"train.txt\", \"a\")\n",
        "for txt in Dataset:\n",
        "  f.write(txt+\"\\n\")\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPN6SLMm6i6"
      },
      "source": [
        "import fasttext\n",
        "fasttext_model = fasttext.train_unsupervised(input='train.txt', dim=300, model='skipgram')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHa8_nw1o2T-"
      },
      "source": [
        "fasttext_model.save_model(\"train.bin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqLLHzm-tWm3"
      },
      "source": [
        "print(fasttext_model.get_nearest_neighbors(\"green\",20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxXSfChEuDEw",
        "outputId": "ffcca7d7-8045-4e03-ee06-dae0e11c9b9e"
      },
      "source": [
        "print(len(fasttext_model['green']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxAdK2NKjBp1",
        "outputId": "f44996bb-aad9-4b4e-8775-126b03dd6c07"
      },
      "source": [
        "# G\n",
        "cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn6Y9WZhtTzq"
      },
      "source": [
        "# Files Reading, Pre-Processing and Calculation of TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbf9A6XIfZ_H"
      },
      "source": [
        "# Extracting data\n",
        "\n",
        "path = './webkb2'\n",
        "files = []\n",
        "labels = []\n",
        "for r, d, f in os.walk(path):\n",
        "\n",
        "  for file in f:\n",
        "      file_temp = \"\"\n",
        "      label = r.rsplit('/', 1)[1]\n",
        "      p = os.path.join(r, file)\n",
        "      file_temp = open(p,'r', errors='ignore').read()\n",
        "      soup = BeautifulSoup(file_temp, 'html.parser')\n",
        "      body = soup.text\n",
        "      body = str(np.char.lower(body))\n",
        "\n",
        "      files.append(body)\n",
        "      labels.append(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k28-aXrigOkJ",
        "outputId": "f2240f42-5254-4e0a-eecd-310c94c89a32"
      },
      "source": [
        "print(set(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cornell', 'texas', 'washington', 'wisconsin'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "eZCAcYf21IcE",
        "outputId": "670d25a2-87a3-4c4c-ff3d-5485f7fc7f16"
      },
      "source": [
        "print(files[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'date: thu, 07 nov 1996 19:16:49 gmt\\nserver: ncsa/1.5\\ncontent-type: text/html\\nlast-modified: fri, 11 oct 1996 18:05:48 gmt\\ncontent-length: 2322\\n\\n\\n\\ntodd turnidge\\n\\n\\n\\n\\ntodd douglas turnidge\\n\\nschool\\n\\ncomputer sciences department\\nuniversity of wisconsin - madison\\n1210 w. dayton st.\\nmadison, wi  53706\\n(608) 262-6612\\n\\nhome\\n\\n\\n1124 milton st.\\nmadison, wi  53715\\n(608) 250-0699\\n\\n\\n\\ni am a graduate student in the \\ndepartment of computer sciences\\nat the university of wisconsin,\\nmadison.  i have been here for two years.  i am working with professor\\nthomas reps studying\\nprogramming languages.\\ni teach a section of cs302.\\ni hold a bs in mathematics\\nand an ms in computer\\nscience from case western reserve\\nuniversity, which is located in  cleveland,\\nohio.\\ni am originally from kent, ohio.  my\\nfamily lives there.\\n\\nthey say you can judge a man by the company he keeps.  click here for enough evidence to put me away for a long time.\\nsome amusements for you.\\nsome shortcuts for me.\\n\\nlast modified: fri oct 11 13:05:48 1996 by todd turnidge\\n\\nturnidge@cs.wisc.edu\\n\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75GvxFsyomei"
      },
      "source": [
        "# Pre-processing\n",
        "\n",
        "stemFeatures = {}\n",
        "# test_list = ['Date: ', 'Server: ', 'Content-type: ', 'Content-length: ', 'Last-modified: ', 'From: ', 'Subject: '] \n",
        "test_list = [] \n",
        "# remove punctuation from each word\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "preprocessed_lines=[]\n",
        "for doc in files: \n",
        "  lines = list(filter(bool, doc.splitlines()))\n",
        "  temp_line=\"\"\n",
        "  for line in lines:\n",
        "    if not any(map(line.__contains__, test_list)):\n",
        "      # convert to lower case\n",
        "      line = str(np.char.lower(line))\n",
        "      line = re.sub(r'\\d+', '', line)\n",
        "      # text_tokens = word_tokenize(line)\n",
        "      text_tokens = re.sub(\"-\", \"\", line)\n",
        "      text_tokens = re.split(r'\\W+', line)\n",
        "      for word in text_tokens: \n",
        "          # remove remaining tokens that are not alphabetic and filter out sopwords\n",
        "          if ((word not in stop_words) and (word.isalpha())): \n",
        "              word = wordnet_lemmatizer.lemmatize(word)\n",
        "              # stemFeatures[word] = porter.stem(word)\n",
        "              # word = porter.stem(word)\n",
        "              temp_line = temp_line + \" \" + word.translate(table)\n",
        "  preprocessed_lines.append(temp_line.strip()) \n",
        "\n",
        "files = preprocessed_lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "hJSexcfolqyr",
        "outputId": "5e86f3e0-5cff-406d-c38f-1c89537850ee"
      },
      "source": [
        "print(files[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'date thu nov gmt server ncsa content type text html last modified fri oct gmt content length todd turnidge todd douglas turnidge school computer science department university wisconsin madison w dayton st madison wi home milton st madison wi graduate student department computer science university wisconsin madison two year working professor thomas rep studying programming language teach section c hold b mathematics m computer science case western reserve university located cleveland ohio originally kent ohio family life say judge man company keep click enough evidence put away long time amusement shortcut last modified fri oct todd turnidge turnidge c wisc edu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq0eOGK2l68S"
      },
      "source": [
        "# tfâ€“idf\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', sublinear_tf=True, strip_accents='unicode',\n",
        "    analyzer='word', token_pattern=r'\\w{2,}', \n",
        "    ngram_range=(1, 1), max_features=100).fit(files)    # no. of features\n",
        "X = vectorizer.fit_transform(files)\n",
        "features = vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYhN_Ieg7AMS",
        "outputId": "6faa4304-aef2-442a-ff5e-d0f2fe4a7599"
      },
      "source": [
        "print(features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['acm', 'address', 'algorithm', 'analysis', 'application', 'architecture', 'assignment', 'austin', 'available', 'based', 'cern', 'class', 'computer', 'computing', 'conference', 'content', 'cornell', 'course', 'cse', 'data', 'database', 'date', 'department', 'design', 'distributed', 'edu', 'email', 'engineering', 'exam', 'fall', 'file', 'gmt', 'graduate', 'group', 'home', 'homework', 'hour', 'html', 'image', 'information', 'jan', 'language', 'lecture', 'length', 'link', 'list', 'machine', 'madison', 'mail', 'memory', 'mime', 'modified', 'monday', 'ncsa', 'network', 'new', 'note', 'nov', 'object', 'oct', 'office', 'operating', 'page', 'paper', 'parallel', 'performance', 'phone', 'postscript', 'problem', 'proceeding', 'program', 'programming', 'project', 'publication', 'report', 'research', 'science', 'section', 'server', 'software', 'solution', 'student', 'texas', 'text', 'th', 'thu', 'time', 'tuesday', 'type', 'university', 'use', 'using', 'utexas', 'version', 'washington', 'web', 'wednesday', 'wisc', 'wisconsin', 'work']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wS4qoFIfM6u"
      },
      "source": [
        "# H\n",
        "def purity(labels, clustered):\n",
        "    \n",
        "    cluster_ids = set(clustered)\n",
        "\n",
        "    N = len(clustered)\n",
        "    majority_sum = 0  \n",
        "    for cl in cluster_ids:\n",
        "        labels_cl = Counter(l for l, c in zip(labels, clustered) if c == cl)\n",
        "        majority_sum += max(labels_cl.values())\n",
        "\n",
        "    return majority_sum / N"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orm5SpX0gTHo"
      },
      "source": [
        "dense = X.todense()\n",
        "denselist = dense.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPZPzwTEgySV"
      },
      "source": [
        "# Tf-Idf Purity Calculation\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, random_state=0)\n",
        "clustered_docs = kmeans.fit_predict(X)\n",
        "\n",
        "\n",
        "print(purity(labels, clustered_docs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVWKN0uUtjxp"
      },
      "source": [
        "# Expand with Embedding Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n61nFC5JlSIg"
      },
      "source": [
        "# Replacing every feature with its fasttext representation\n",
        "\n",
        "fasttext_docs = []\n",
        "fasttext_dist = []\n",
        "\n",
        "for file in denselist:\n",
        "  dist = 0\n",
        "  counter = 0\n",
        "  long_vector = []\n",
        "  for feature in features:\n",
        "      if(file[counter] == 0.0):   # feature not in doc\n",
        "          long_vector.append(0.0)\n",
        "          counter+=1\n",
        "      else:\n",
        "          try:\n",
        "              long_vector.append(file[counter]*(sum(fasttext_model[feature]) / 300))   # feature representation available\n",
        "          except:\n",
        "              long_vector.append(file[counter])   # feature representation not available\n",
        "          counter+=1\n",
        "  dist = math.sqrt(np.sum(np.power((np.array(file)-np.array(long_vector)),2)))\n",
        "  long_vector = np.array(long_vector) * dist\n",
        "  fasttext_dist.append(dist)\n",
        "  fasttext_docs.append(long_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2M5wVsSn11r"
      },
      "source": [
        "# Replacing every feature with its word2vec representation\n",
        "\n",
        "word2vec_docs = []\n",
        "word2vec_dist = []\n",
        "\n",
        "for file in denselist:\n",
        "  dist = 0\n",
        "  counter = 0\n",
        "  long_vector = []\n",
        "  for feature in features:\n",
        "      if(file[counter] == 0.0):   # feature not in doc\n",
        "          long_vector.append(0.0)\n",
        "          counter+=1\n",
        "      else:\n",
        "          try:\n",
        "              long_vector.append(file[counter]*(sum(word2vec_model[feature]) / 300))   # feature representation available\n",
        "          except:\n",
        "              long_vector.append(file[counter])   # feature representation not available\n",
        "          counter+=1\n",
        "  dist = math.sqrt(np.sum(np.power((np.array(file)-np.array(long_vector)),2)))\n",
        "  long_vector = np.array(long_vector) * dist\n",
        "  word2vec_dist.append(dist)\n",
        "  word2vec_docs.append(long_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R6iWJa5rOKi"
      },
      "source": [
        "# Replacing every feature with its glove representation\n",
        "\n",
        "glove_docs = []\n",
        "glove_dist = []\n",
        "\n",
        "for file in denselist:\n",
        "  dist = 0\n",
        "  counter = 0\n",
        "  long_vector = []\n",
        "  for feature in features:\n",
        "      if(file[counter] == 0.0):   # feature not in doc\n",
        "          long_vector.append(0.0)\n",
        "          counter+=1\n",
        "      else:\n",
        "          try:\n",
        "              long_vector.append(file[counter]*(sum(glove_model[feature]) / 300))   # feature representation available\n",
        "          except:\n",
        "              long_vector.append(file[counter])   # feature representation not available\n",
        "          counter+=1\n",
        "  dist = math.sqrt(np.sum(np.power((np.array(file)-np.array(long_vector)),2)))\n",
        "  long_vector = np.array(long_vector) * dist\n",
        "  glove_dist.append(dist)\n",
        "  glove_docs.append(long_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xxf7hBNjsnzE"
      },
      "source": [
        "# Replacing every feature with its u_s_e_ representation\n",
        "\n",
        "u_s_e_docs = []\n",
        "u_s_e_dist = []\n",
        "\n",
        "for file in denselist:\n",
        "  dist = 0\n",
        "  counter = 0\n",
        "  long_vector = []\n",
        "  for feature in features:\n",
        "      if(file[counter] == 0.0):   # feature not in doc\n",
        "          long_vector.append(0.0)\n",
        "          counter+=1\n",
        "      else:\n",
        "          try:\n",
        "              long_vector.append(file[counter]*(sum(u_s_e_model([feature])[0]) / 512))   # feature representation available\n",
        "          except:\n",
        "              long_vector.append(file[counter])   # feature representation not available\n",
        "          counter+=1\n",
        "  dist = math.sqrt(np.sum(np.power((np.array(file)-np.array(long_vector)),2)))\n",
        "  long_vector = np.array(long_vector) * dist\n",
        "  u_s_e_dist.append(dist)\n",
        "  u_s_e_docs.append(long_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4KfhQsgu4WF"
      },
      "source": [
        "# Docs Conversion, Clustering and Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHcE2PnLWdb3"
      },
      "source": [
        "# Multiplying docs with distance coefficient from raw doc\n",
        "total = np.array(fasttext_dist) + np.array(word2vec_dist) + np.array(glove_dist) + np.array(u_s_e_dist)\n",
        "final_docs = []\n",
        "\n",
        "for i in range(len(glove_docs)):\n",
        "  total[i] = 1.0 if total[i] == 0.0 else total[i]\n",
        "  fasttext_docs[i] = np.array(fasttext_docs[i]) / total[i]\n",
        "  word2vec_docs[i] = np.array(word2vec_docs[i]) / total[i]\n",
        "  glove_docs[i] = np.array(glove_docs[i]) / total[i]\n",
        "  u_s_e_docs[i] = np.array(u_s_e_docs[i]) / total[i]\n",
        "\n",
        "  final_docs.append(np.array((fasttext_docs[i] + word2vec_docs[i] + glove_docs[i] + u_s_e_docs[i]) / 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwKPQrhekls9"
      },
      "source": [
        "# Conversion to unit vector\n",
        "for i in range(len(final_docs)):\n",
        "  magnitude = np.linalg.norm(np.array(final_docs[i]))\n",
        "  magnitude = 1.0 if magnitude == 0.0 else magnitude\n",
        "  final_docs[i] = np.array(final_docs[i]) / magnitude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpD4j6zpqsvw"
      },
      "source": [
        "# K-Means Algorithm\n",
        "\n",
        "clustered_docs = []\n",
        "# I\n",
        "def kmeans(s1, s2, s3, s4, docs):\n",
        "  clusters = [[], [], [], []]\n",
        "\n",
        "  for i in range(len(docs)):\n",
        "\n",
        "    d1 = 0\n",
        "    d2 = 0\n",
        "    d3 = 0\n",
        "    d4 = 0\n",
        "  \n",
        "    d1 = math.sqrt(np.sum(np.power((np.array(s1)-np.array(docs[i])),2)))\n",
        "    d2 = math.sqrt(np.sum(np.power((np.array(s2)-np.array(docs[i])),2)))\n",
        "    d3 = math.sqrt(np.sum(np.power((np.array(s3)-np.array(docs[i])),2)))\n",
        "    d4 = math.sqrt(np.sum(np.power((np.array(s4)-np.array(docs[i])),2)))\n",
        "\n",
        "    mini = min(d1, d2, d3, d4)\n",
        "    if(mini == d1):\n",
        "      clusters[0].append(docs[i])\n",
        "      clustered_docs.append(0)\n",
        "    elif(mini == d2):\n",
        "      clusters[1].append(docs[i])\n",
        "      clustered_docs.append(1)\n",
        "    elif(mini == d3):\n",
        "      clusters[2].append(docs[i])\n",
        "      clustered_docs.append(2)\n",
        "    elif(mini == d4):\n",
        "      clusters[3].append(docs[i])\n",
        "      clustered_docs.append(3)\n",
        "      \n",
        "  return clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDyFFEvTeGmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e56588b-020a-4462-f603-49a906f60bc9"
      },
      "source": [
        "# Calculating Inter and Intra Cluster Similarity on every iteration\n",
        "\n",
        "rand = random.sample(range(0, len(files)), 4)\n",
        "seeds = []\n",
        "seeds.append(final_docs[rand[0]])\n",
        "seeds.append(final_docs[rand[1]])\n",
        "seeds.append(final_docs[rand[2]])\n",
        "seeds.append(final_docs[rand[3]])\n",
        "\n",
        "for i in range(30):\n",
        "  clustered_docs = []\n",
        "  clusters = kmeans(seeds[0], seeds[1], seeds[2], seeds[3], final_docs)\n",
        "  seeds[0] = np.array([0]*100)\n",
        "  seeds[1] = np.array([0]*100)\n",
        "  seeds[2] = np.array([0]*100)\n",
        "  seeds[3] = np.array([0]*100)\n",
        "  \n",
        "  for doc in clusters[0]:\n",
        "    seeds[0] = np.array(seeds[0]) + np.array(doc)\n",
        "  seeds[0] /= len(clusters[0])\n",
        "\n",
        "  for doc in clusters[1]:\n",
        "   seeds[1] = np.array(seeds[1]) + np.array(doc)\n",
        "  seeds[1] /= len(clusters[1])\n",
        "\n",
        "  for doc in clusters[2]:\n",
        "   seeds[2] = np.array(seeds[2]) + np.array(doc)\n",
        "  seeds[2] /= len(clusters[2])\n",
        "\n",
        "  for doc in clusters[3]:\n",
        "   seeds[3] = np.array(seeds[3]) + np.array(doc)\n",
        "  seeds[3] /= len(clusters[3])\n",
        "\n",
        "  print(\"Purity :\", purity(labels, clustered_docs))\n",
        "\n",
        "  silhouette_avg = silhouette_score(final_docs, np.array(clustered_docs))\n",
        "  print(\"The average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "  calinski_harabasz_avg = calinski_harabasz_score(final_docs, np.array(clustered_docs))\n",
        "  print(\"The average calinski_harabasz_score is :\", calinski_harabasz_avg)\n",
        "  print(\"*********************************\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Purity : 0.5609532538955087\n",
            "The average silhouette_score is : 0.06979149307570438\n",
            "The average calinski_harabasz_score is : 195.53914429702326\n",
            "*********************************\n",
            "Purity : 0.8625114573785518\n",
            "The average silhouette_score is : 0.5850651312232006\n",
            "The average calinski_harabasz_score is : 791.0590833184986\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n",
            "Purity : 0.8661778185151238\n",
            "The average silhouette_score is : 0.672798505343015\n",
            "The average calinski_harabasz_score is : 1294.4585019256763\n",
            "*********************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTQMQQtGwGRj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}